{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Clean DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('https://alexip-ml.s3.amazonaws.com/stackexchange_812k.csv.gz', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.category.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Unwanted Text Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    " \n",
    "# html tags\n",
    "df['text'] = df.text.apply(lambda t : re.sub(\"<[^>]*>\",' ', t))\n",
    "# line returns\n",
    "df['text'] = df.text.apply(lambda t : re.sub(\"[\\r\\n]+\",' ', t))\n",
    "# urls\n",
    "df['text'] = df.text.apply(lambda t : re.sub(\"http\\S+\",' ', t))\n",
    "# mentions\n",
    "df['text'] = df.text.apply(lambda t : re.sub(\"@\\S+\",' ', t))\n",
    "# latex\n",
    "df['text'] = df.text.apply(lambda t : re.sub(\"\\$[^>]*\\$\",' ', t))\n",
    "# digits\n",
    "df['text'] = df.text.apply(lambda t : re.sub(\"\\d+\",' ', t))\n",
    "# rm some of the punctuation but keep ,.!? and -\n",
    "punctuation = '\"#$%&()*+/:;<=>@[\\\\]^_`{|}~”“'\n",
    "pattern = r\"[{}]\".format(punctuation)\n",
    "df['text'] = df.text.apply(lambda t : re.sub(pattern,' ', t))\n",
    "# multiple spaces\n",
    "df['text'] = df.text.apply(lambda t : re.sub(\"\\s\\s+\",' ', t))\n",
    "# trailing spaces\n",
    "df['text'] = df.text.apply(lambda t : t.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "tokenizer = WordPunctTokenizer()\n",
    "df['tokens'] = df.text.apply(lambda t : tokenizer.tokenize(t.lower()))\n",
    "\n",
    "# Add number of tokens\n",
    "df['n_tokens'] = df.tokens.apply(len)\n",
    "\n",
    "# Remove texts that are too long or too short\n",
    "cleanedTokenizedData = df[(df.n_tokens > 4) & (df.n_tokens < 5000)]\n",
    "cleanedTokenizedData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5).tokens.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tokenized Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenized dataset that's space separated\n",
    "tokenizedDf = pd.read_csv('https://alexip-ml.s3.amazonaws.com/stackexchange_812k.tokenized.csv.gz', compression='gzip')\n",
    "print(df.head())\n",
    "# Get list of tokens\n",
    "tokenizedDf['tokens'] = tokenizedDf.tokens.apply(lambda txt : txt.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizedDf.sample(5).tokens.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Testing and Training DataSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testingDf = cleanedTokenizedData[cleanedTokenizedData.category == 'title'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingDf = cleanedTokenizedData[(cleanedTokenizedData.category == 'post') | (cleanedTokenizedData.category == 'comment')].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-- Training set: {}\\n\".format(trainingDf.shape))\n",
    "print(trainingText.head())\n",
    "\n",
    "print(\"\\n-- Testing set {}\\n\".format(testingDf.shape))\n",
    "print(testingText.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from nltk.util import ngrams, trigrams\n",
    "\n",
    "\n",
    "n = 3 # Trigrams\n",
    "\n",
    "exampleNgrams = ngrams(trainingText.iloc[0].tokens, n, pad_left = True, pad_right = True, left_pad_symbol = \"<s>\", right_pad_symbol=\"</s>\")\n",
    "exampleTrigrams = trigrams(trainingText.iloc[0], pad_left = True, pad_right = True, left_pad_symbol = \"<s>\", right_pad_symbol=\"</s>\")\n",
    "\n",
    "paddedNGrams = ngrams(trainingText.tokens.values, n, pad_left = True, pad_right = True, left_pad_symbol = \"<s>\", right_pad_symbol=\"</s>\")\n",
    "\n",
    "# list(paddedNGrams)\n",
    "\n",
    "list(exampleNgrams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Prefix/Frequency Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "counts = defaultdict(Counter)\n",
    "n = 3 # Trigrams\n",
    "\n",
    "for tokens in trainingDf.tokens.values:\n",
    "    for ngram in ngrams(\n",
    "            tokens,\n",
    "            n,\n",
    "            pad_left = True,\n",
    "            pad_right = True,\n",
    "            left_pad_symbol = \"<s>\",\n",
    "            right_pad_symbol=\"</s>\"):\n",
    "        \n",
    "        prefix = ngram[:n-1]\n",
    "        token = ngram[n-1]\n",
    "        counts[prefix][token] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"we have {} bigrams\".format(len(counts.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "for i in range(5):\n",
    "    prefix = random.choice(list(counts.keys()))\n",
    "    print(\"{}: \\t{}\".format(prefix,counts[prefix]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token / Prefix Probabilities\n",
    "\n",
    "$$p(token / prefix) = \\frac{count(prefix + token)} {count(prefix)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probabilities\n",
    "frequencies = defaultdict(dict)\n",
    "\n",
    "for prefix, tokens in counts.items():\n",
    "    total_count = sum(tokens.values())\n",
    "    for token, count in tokens.items():\n",
    "        frequencies[prefix][token] = count / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    prefix = random.choice(list(frequencies.keys()))\n",
    "    print(\"{}: \\t{}\".format(prefix,frequencies[prefix]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation\n",
    "\n",
    "- takes a bigram (must exist in corpus) as input\n",
    "- generates a new token by sampling the available tokens related to the bigram using the frequency object as distribution\n",
    "- slides the bigram to include the new token\n",
    "- generates a new token based on the new bigram\n",
    "- stops when the text is N tokens long or the latest token is the end of string symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate(text, n_tokens = 20):\n",
    "    for i in range(n_tokens):\n",
    "        prefix = tuple(text.split()[-n + 1:]) # n = 3 since we're looking at trigrams\n",
    "        if len(frequencies[prefix]) == 0: # next word is not loaded in the frequency dictionary\n",
    "            break\n",
    "        candidates = list(frequencies[prefix].keys()) # find tokens that could follow that prefix\n",
    "        probabilities = list(frequencies[prefix].values()) # find probabilities of tokens that can follow prefix\n",
    "        text += ' ' + np.random.choice(candidates, p = probabilities)\n",
    "        if text.endswith('</s>'):\n",
    "            break\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'the model'\n",
    "print()\n",
    "print(generate(text))\n",
    "\n",
    "print()\n",
    "text = 'that distribution'\n",
    "print(generate(text))\n",
    "\n",
    "print()\n",
    "text = 'to determine'\n",
    "print(generate(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an n-gram language model using NLTK\n",
    "https://www.nltk.org/api/nltk.lm.html#module-nltk.lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "import pandas as pd\n",
    "\n",
    "assert(nltk.__version__ >= '3.4') # requires v >= 3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into pandas dataframe, shuffle it and reset the index\n",
    "df = pd.read_csv('https://alexip-ml.s3.amazonaws.com/stackexchange_812k.tokenized.csv.gz', compression='gzip').sample(frac = 1, random_state = 8).reset_index(drop = True)\n",
    "\n",
    "df['tokens'] = df.tokens.apply(lambda txt : txt.split())\n",
    "\n",
    "# Tokenize and divide into training and testing datasetsdf['tokens'] = df.tokens.apply(lambda txt : txt.split())\n",
    "df_train = df[df.category.isin(['post','comment'])].copy()\n",
    "df_test = df[df.category.isin(['title'])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the training data\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline \n",
    "from nltk.lm import MLE\n",
    "from nltk.lm import Vocabulary\n",
    "from nltk.util import ngrams\n",
    "\n",
    "\n",
    "ngrams_degree = 3\n",
    "\n",
    "# train_data = [\n",
    "#     ngrams(t, n= ngrams_degree,\n",
    "#         pad_right=True, pad_left=True,\n",
    "#         left_pad_symbol=\"<s>\", right_pad_symbol=\"</s>\")\n",
    "#     for t in df_train.tokens.values]\n",
    "\n",
    "# words = [word for sent in df_train.tokens.values for word in sent]\n",
    "# words.extend([\"<s>\", \"</s>\"])\n",
    "train, vocab = padded_everygram_pipeline(ngrams_degree, df_train.tokens)\n",
    "# vocab = Vocabulary(words, unk_cutoff = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "model = MLE(ngrams_degree)\n",
    "# print(len(model.vocab))\n",
    "\n",
    "# fit the model\n",
    "model.fit(train, vocab)\n",
    "# print(len(model.vocab))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out of Vocabulary (OOV) - Laplace smoothing\n",
    "The vocabulary helps us handle words that have not occurred during training\n",
    "To remediate to that problem we can artificially assign a probability (although a very low one) to missing ngrams and tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.vocab.lookup([\"aliens\", \"from\", \"Mars\"]))\n",
    "print(model.counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring a Model\n",
    "Indicates how probable words are in certain contexts. This being MLE, the model returns the item’s relative frequency as its score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example we want to know what is the chance that “has” is preceded by “the model”.\n",
    "model.score(\"has\", [\"the\", \"model\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity\n",
    "- Measures the quality of the model.\n",
    "\n",
    "- The idea is to estimate the probability of a test sentence given the model, so an uncommon sentence should be less probable than a common one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = the difference between the two approaches is discussed here.\n",
    "model.perplexity(test)\n",
    "test = the difference between the two approaches is discussed here\n",
    "model.perplexity(test)\n",
    "test = the difference between the two approaches\n",
    "model.perplexity(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text\n",
    "model.generate(5, random_seed=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
